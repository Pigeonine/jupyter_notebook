{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d2f7f5e-0a93-4353-8ec7-ace6f1ae3854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "with open('.env', 'w') as f:\n",
    "    f.write('OPENAI_API_KEY=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')\n",
    "\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"test\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3e542c2-e5ff-43dc-af15-43a4c869a745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "# 설치: pip install python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62ca63be-253f-4971-8665-52fcc03ee70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[API KEY]\n",
      "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\n",
      "[LANGCHAIN_TRACING_V2]\n",
      "true\n",
      "[LANGCHAIN_ENDPOINT]\n",
      "https://api.smith.langchain.com\n",
      "[LANGCHAIN_API_KEY]\n",
      "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\n",
      "[LANGCHAIN_PROJECT]\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "print(f\"[API KEY]\\n{os.environ['OPENAI_API_KEY']}\")\n",
    "print(f\"[LANGCHAIN_TRACING_V2]\\n{os.environ['LANGCHAIN_TRACING_V2']}\")\n",
    "print(f\"[LANGCHAIN_ENDPOINT]\\n{os.environ['LANGCHAIN_ENDPOINT']}\")\n",
    "print(f\"[LANGCHAIN_API_KEY]\\n{os.environ['LANGCHAIN_API_KEY']}\")\n",
    "print(f\"[LANGCHAIN_PROJECT]\\n{os.environ['LANGCHAIN_PROJECT']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c85317c-bdfb-498f-a838-0c621610b7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "RAG\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"RAG\")\n",
    "\n",
    "# set_enable=False 로 지정하면 추적을 하지 않습니다.\n",
    "#logging.langsmith(\"Test\", set_enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "efbc0f05-2fa4-4631-ad6e-d04a8840b5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, PyPDFDirectoryLoader\n",
    "from langchain_community.vectorstores import FAISS, Chroma\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings,FastEmbedEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.llms import Ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dc74e118-ebc0-4bce-a4b3-4d9fa19e4bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단계 1: 문서 로드(Load Documents)\n",
    "# loader = PyMuPDFLoader(\"data/SPRI_AI_Brief_2023년12월호_F.pdf\") #단일 pdf\n",
    "loader = PyPDFDirectoryLoader(\"./data\") #여러개 pdf\n",
    "docs = loader.load()\n",
    "\n",
    "# 단계 2: 문서 분할(Split Documents)\n",
    "#text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100, separator=\" \")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# 단계 3: 임베딩(Embedding) 생성\n",
    "embeddings = OpenAIEmbeddings() #OpenAi 기반\n",
    "#embeddings = HuggingFaceBgeEmbeddings() #HuggingFace 기반(free)\n",
    "#embeddings =FastEmbedEmbeddings() #open source\n",
    "\n",
    "# 단계 4: DB 생성(Create DB) 및 저장.\n",
    "vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings) #FAISS사용\n",
    "#vectorstore = Chroma.from_documents(documents=split_documents, embedding=embeddings) #chroma 사용\n",
    "\n",
    "\n",
    "#vectorstore.save_local(\"./vec\") #local 저장\n",
    "\n",
    "#vectorstore = FAISS.load_local(\"./vec\", embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aa03c502-f05d-4ad4-b808-21e4fb2036e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 단계 5: 검색기(Retriever) 생성\n",
    "# 문서에 포함되어 있는 정보를 검색하고 생성합니다.\n",
    "retriever = vectorstore.as_retriever()\n",
    "# 단계 6: 프롬프트 생성(Create Prompt)\n",
    "# 프롬프트를 생성합니다.\n",
    "# prompt = PromptTemplate.from_template(\n",
    "#     \"\"\"You are an assistant for question-answering tasks. \n",
    "# Use the following pieces of retrieved context to answer the question. \n",
    "# If you don't know the answer, just say that you don't know. \n",
    "# Answer in Korean.\n",
    "\n",
    "# #Question: \n",
    "# {question} \n",
    "# #Context: \n",
    "# {context} \n",
    "\n",
    "# #Answer:\"\"\"\n",
    "# )\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\") #hub에서 가져온 rag 프롬프트\n",
    "\n",
    "# 단계 7: 언어모델(LLM) 생성\n",
    "# 모델(LLM) 을 생성합니다.\n",
    "#llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.2)\n",
    "#llm = Ollama(model=\"llama3.2:1b\")\n",
    "\n",
    "# 단계 8: 체인(Chain) 생성\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "08cb79b6-e97a-4db2-8c3b-b3b5dbe5402f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeYO is a method designed to improve sample selection and weighting in predictions by utilizing a new confidence metric called PLPD. The experiments conducted aim to evaluate DeYO's performance against baseline methods in various scenarios, including real-world biased situations. The results indicate that combining different weighting terms enhances overall performance.\n"
     ]
    }
   ],
   "source": [
    "question = \"DeYO 논문 내용을 요약해줘\"\n",
    "response = chain.invoke(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "24b1de94-07f4-4d19-9e2b-3a905a05e201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoTTA (Continual Test-Time Adaptation) is a method designed to improve the performance of pre-trained models during test-time by reducing error accumulation and preventing catastrophic forgetting. It utilizes weight-averaged and augmentation-averaged predictions for accuracy and stochastically restores some neurons to their pre-trained weights to maintain source knowledge. The effectiveness of CoTTA has been validated across four classification tasks and one segmentation task, outperforming existing methods.\n"
     ]
    }
   ],
   "source": [
    "question = \"CoTTA 논문 내용을 요약해줘\"\n",
    "response = chain.invoke(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2fec5cf6-4b02-4c97-aab5-f430c4dad8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tent 논문에 대한 구체적인 내용은 제공된 문서에서 확인할 수 없습니다. 그러나 관련된 AI 모델과 기술에 대한 정보가 포함되어 있습니다. 추가적인 세부사항이 필요하다면 다른 자료를 참고해야 할 것 같습니다.\n"
     ]
    }
   ],
   "source": [
    "question = \"Tent 논문 내용을 요약해줘\"\n",
    "response = chain.invoke(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c564f37a-8a78-4ae2-9e23-4c75fca04c61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b3a62c-982f-4a7b-a32c-d20531620a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
